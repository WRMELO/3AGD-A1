{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de91aeda",
   "metadata": {},
   "source": [
    "Lê o dicionário features_padronizadas.csv (colunas: nome_original, nome_pt, dimensao, exemplo_valor);\n",
    "\n",
    "Varre as pastas principais (as da sua imagem);\n",
    "\n",
    "Para cada CSV encontrado:\n",
    "\n",
    "Faz backup do original como *_old.csv (se ainda não existir);\n",
    "\n",
    "Renomeia apenas as colunas que tiverem mapeamento para português (nome_pt);\n",
    "\n",
    "Mantém colunas não mapeadas como estão (simples);\n",
    "\n",
    "Salva de volta com o nome original;\n",
    "\n",
    "Registra no inventário de auditoria uma linha contendo todas as nome_pt daquele arquivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "058700cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dicionário: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\\features_padronizadas.csv | Itens: 150 | SHA256: 73078b08b9ab591a9871889bdcb0e69c04329ab83d069534ffd21dc19ef8a5df\n",
      "Arquivos a processar: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizando features: 100%|██████████| 26/26 [00:03<00:00,  6.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concluído.\n",
      "Inventário: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\\outputs\\auditoria_normalizacao_features.csv\n",
      "                                         arquivo_relativo                  acao  qtd_mapeadas  qtd_nao_mapeadas\n",
      "                     data\\curated\\a1_physics_informed.csv backup_e_normalizacao           150                 0\n",
      "            data\\curated\\a1_physics_informed_enriched.csv backup_e_normalizacao           150                 4\n",
      "             data\\curated\\a1_physics_informed_proxies.csv backup_e_normalizacao           150                16\n",
      "    outputs\\baseline_datasets\\compare_baseline_vs_off.csv     somente_auditoria             0                 4\n",
      "   outputs\\baseline_datasets\\physics_baseline_proxies.csv backup_e_normalizacao           150                16\n",
      "outputs\\baseline_datasets\\physics_offbaseline_proxies.csv backup_e_normalizacao           150                16\n",
      "             outputs\\baseline_datasets\\stats_baseline.csv     somente_auditoria             0                 9\n",
      "          outputs\\baseline_datasets\\stats_offbaseline.csv     somente_auditoria             0                 9\n",
      "                             outputs\\baseline_janelas.csv     somente_auditoria             0                 5\n",
      "                outputs\\baseline_janelas_intersection.csv     somente_auditoria             0                 5\n",
      "                                outputs\\baseline_mask.csv backup_e_normalizacao             1                 1\n",
      "         outputs\\comparacao_colunas_staged_vs_curated.csv     somente_auditoria             0                 6\n",
      "        outputs\\eda\\correlacoes_altas_20250809_115344.csv     somente_auditoria             0                 3\n",
      "        outputs\\eda\\matriz_correlacao_20250809_115344.csv backup_e_normalizacao           149                 1\n",
      "                      outputs\\inventario_classificado.csv     somente_auditoria             0                 7\n",
      "                          outputs\\inventario_features.csv     somente_auditoria             0                 3\n",
      "                      outputs\\inventario_temperaturas.csv     somente_auditoria             0                 6\n",
      "            outputs\\inventario_temperaturas_inidcadas.csv     somente_auditoria             0                 1\n",
      "         outputs\\pedra\\A1_SECONDARIES_FOR_PEDRA_MODEL.csv backup_e_normalizacao             4                 6\n",
      "                    outputs\\pedra\\A1_SECONDARIES_REFS.csv     somente_auditoria             0                 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Normalização de nomes de features para PT-BR (com backup simples) ===\n",
    "# Requisitos: pandas, tqdm\n",
    "# O que faz:\n",
    "#  - Lê dicionário canônico \"features_padronizadas.csv\"\n",
    "#  - Varre as pastas principais e processa todos .csv\n",
    "#  - Para cada arquivo:\n",
    "#      * cria BACKUP simples: <nome>_old.csv (se não existir)\n",
    "#      * renomeia colunas que tiverem mapeamento nome_original -> nome_pt\n",
    "#      * mantém demais colunas intactas\n",
    "#      * salva com o NOME ORIGINAL\n",
    "#  - Atualiza inventário: outputs/auditoria_normalizacao_features.csv\n",
    "#\n",
    "# Observação: não processa arquivos *_old.csv nem o próprio inventário.\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import hashlib\n",
    "import datetime as dt\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "BASE_DIR = Path(r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\")\n",
    "CSV_DIC  = BASE_DIR / \"features_padronizadas.csv\"\n",
    "\n",
    "GLOBS = [\n",
    "    BASE_DIR / \"data\" / \"curated\" / \"*.csv\",\n",
    "    BASE_DIR / \"outputs\" / \"*.csv\",\n",
    "    BASE_DIR / \"outputs\" / \"baseline_datasets\" / \"*.csv\",\n",
    "    BASE_DIR / \"outputs\" / \"pedra\" / \"*.csv\",\n",
    "    BASE_DIR / \"outputs\" / \"eda\" / \"*.csv\",\n",
    "    BASE_DIR / \"outputs\" / \"indices\" / \"*.csv\",\n",
    "]\n",
    "\n",
    "OUT_DIR       = BASE_DIR / \"outputs\"\n",
    "OUT_AUDITORIA = OUT_DIR / \"auditoria_normalizacao_features.csv\"\n",
    "\n",
    "# ---------- FUNÇÕES AUX ----------\n",
    "def sha256_file(p: Path) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with p.open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def norm_key(s: str) -> str:\n",
    "    return (s or \"\").strip().lower()\n",
    "\n",
    "def read_csv_header_any_encoding(p: Path):\n",
    "    # Lê só o cabeçalho (nrows=0) tentando encodings comuns\n",
    "    for enc in (\"utf-8\", \"utf-8-sig\", \"latin1\"):\n",
    "        try:\n",
    "            return pd.read_csv(p, nrows=0, encoding=enc), enc\n",
    "        except Exception:\n",
    "            continue\n",
    "    # fallback: sem encoding explícito\n",
    "    return pd.read_csv(p, nrows=0), None\n",
    "\n",
    "def read_csv_any_encoding(p: Path):\n",
    "    for enc in (\"utf-8\", \"utf-8-sig\", \"latin1\"):\n",
    "        try:\n",
    "            return pd.read_csv(p, encoding=enc, low_memory=False), enc\n",
    "        except Exception:\n",
    "            continue\n",
    "    return pd.read_csv(p, low_memory=False), None\n",
    "\n",
    "# ---------- CARREGAR DICIONÁRIO ----------\n",
    "if not CSV_DIC.exists():\n",
    "    raise FileNotFoundError(f\"Dicionário não encontrado: {CSV_DIC}\")\n",
    "\n",
    "dic_hash = sha256_file(CSV_DIC)\n",
    "df_dic = pd.read_csv(CSV_DIC, dtype=str).fillna(\"\")\n",
    "df_dic[\"nome_original\"] = df_dic[\"nome_original\"].map(norm_key)\n",
    "df_dic[\"nome_pt\"]       = df_dic[\"nome_pt\"].astype(str).str.strip()\n",
    "MAP_ORIG_TO_PT = dict(zip(df_dic[\"nome_original\"], df_dic[\"nome_pt\"]))\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- COLETAR ALVOS ----------\n",
    "alvos = []\n",
    "for g in GLOBS:\n",
    "    alvos.extend(sorted(g.parent.glob(g.name)))\n",
    "\n",
    "# Filtra CSVs e remove duplicados, *_old e o inventário\n",
    "seen = set()\n",
    "arquivos = []\n",
    "for p in alvos:\n",
    "    if p.suffix.lower() != \".csv\": \n",
    "        continue\n",
    "    if p.name.endswith(\"_old.csv\"):\n",
    "        continue\n",
    "    if p.resolve() == OUT_AUDITORIA.resolve():\n",
    "        continue\n",
    "    key = str(p.resolve())\n",
    "    if key not in seen and p.exists():\n",
    "        seen.add(key)\n",
    "        arquivos.append(p)\n",
    "\n",
    "print(f\"Dicionário: {CSV_DIC} | Itens: {len(MAP_ORIG_TO_PT)} | SHA256: {dic_hash}\")\n",
    "print(f\"Arquivos a processar: {len(arquivos)}\")\n",
    "\n",
    "# ---------- PROCESSAMENTO ----------\n",
    "registros = []\n",
    "agora = dt.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "for csv_path in tqdm(arquivos, desc=\"Normalizando features\"):\n",
    "    try:\n",
    "        # Lê cabeçalho para decidir mapeamento\n",
    "        df_head, enc_head = read_csv_header_any_encoding(csv_path)\n",
    "        cols = list(df_head.columns)\n",
    "\n",
    "        # Prepara novo nome (mapa parcial: só mapeia quem existir no dicionário)\n",
    "        novo_nome = {}\n",
    "        mapped_pt = []\n",
    "        unmapped = []\n",
    "        for c in cols:\n",
    "            key = norm_key(c)\n",
    "            if key in MAP_ORIG_TO_PT:\n",
    "                pt = MAP_ORIG_TO_PT[key]\n",
    "                novo_nome[c] = pt\n",
    "                mapped_pt.append(pt)\n",
    "            else:\n",
    "                unmapped.append(c)\n",
    "\n",
    "        # Se nada foi mapeado, apenas audita e segue\n",
    "        if not novo_nome:\n",
    "            registros.append({\n",
    "                \"arquivo_relativo\": str(csv_path.relative_to(BASE_DIR)),\n",
    "                \"total_colunas\": len(cols),\n",
    "                \"qtd_mapeadas\": 0,\n",
    "                \"qtd_nao_mapeadas\": len(cols),\n",
    "                \"todas_features_pt\": \"\",\n",
    "                \"orig_nao_mapeadas\": \";\".join(unmapped),\n",
    "                \"hash_dicionario\": dic_hash,\n",
    "                \"datahora_processo\": agora,\n",
    "                \"acao\": \"somente_auditoria\",\n",
    "                \"status\": \"ok\"\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # BACKUP simples (uma única vez)\n",
    "        backup_path = csv_path.with_name(csv_path.stem + \"_old.csv\")\n",
    "        if not backup_path.exists():\n",
    "            shutil.copy2(csv_path, backup_path)\n",
    "\n",
    "        # Lê tudo, renomeia parcialmente e salva de volta com NOME ORIGINAL\n",
    "        df_full, enc_full = read_csv_any_encoding(csv_path)\n",
    "        df_full_ren = df_full.rename(columns=novo_nome)\n",
    "        df_full_ren.to_csv(csv_path, index=False, encoding=enc_full or \"utf-8\")\n",
    "\n",
    "        registros.append({\n",
    "            \"arquivo_relativo\": str(csv_path.relative_to(BASE_DIR)),\n",
    "            \"total_colunas\": len(cols),\n",
    "            \"qtd_mapeadas\": len(novo_nome),\n",
    "            \"qtd_nao_mapeadas\": len(unmapped),\n",
    "            # todas as features PT daquele arquivo (apenas as mapeadas)\n",
    "            \"todas_features_pt\": \";\".join(mapped_pt),\n",
    "            \"orig_nao_mapeadas\": \";\".join(unmapped),\n",
    "            \"hash_dicionario\": dic_hash,\n",
    "            \"datahora_processo\": agora,\n",
    "            \"acao\": \"backup_e_normalizacao\",\n",
    "            \"status\": \"ok\"\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        registros.append({\n",
    "            \"arquivo_relativo\": str(csv_path.relative_to(BASE_DIR)),\n",
    "            \"total_colunas\": None,\n",
    "            \"qtd_mapeadas\": None,\n",
    "            \"qtd_nao_mapeadas\": None,\n",
    "            \"todas_features_pt\": \"\",\n",
    "            \"orig_nao_mapeadas\": \"\",\n",
    "            \"hash_dicionario\": dic_hash,\n",
    "            \"datahora_processo\": agora,\n",
    "            \"acao\": \"erro\",\n",
    "            \"status\": str(e)\n",
    "        })\n",
    "\n",
    "# ---------- INVENTÁRIO / AUDITORIA ----------\n",
    "df_aud = pd.DataFrame(registros).sort_values(by=[\"arquivo_relativo\"], kind=\"mergesort\")\n",
    "df_aud.to_csv(OUT_AUDITORIA, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"\\nConcluído.\")\n",
    "print(f\"Inventário: {OUT_AUDITORIA}\")\n",
    "print(df_aud[[\"arquivo_relativo\",\"acao\",\"qtd_mapeadas\",\"qtd_nao_mapeadas\"]].head(20).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42f17459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NÃO mapeadas distintas encontradas: 84\n",
      "Propostas salvas em: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\\outputs\\propostas_novas_features.csv  |  Novas entradas: 84\n",
      "Backup do dicionário criado: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\\features_padronizadas_old.csv\n",
      "Dicionário atualizado: C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\\features_padronizadas.csv  |  Linhas totais: 234\n"
     ]
    }
   ],
   "source": [
    "# === Expandir dicionário: coletar NÃO MAPEADAS do inventário e acrescentar ao features_padronizadas.csv ===\n",
    "# O que faz:\n",
    "#  1) Lê outputs/auditoria_normalizacao_features.csv para descobrir colunas 'orig_nao_mapeadas'\n",
    "#  2) Deduplica e lista em quais arquivos cada coluna aparece\n",
    "#  3) Gera \"sugestao_nome_pt\" com as MESMAS regras determinísticas anteriores\n",
    "#  4) Coleta um exemplo de valor (primeiro não nulo) de alguma fonte onde a coluna exista\n",
    "#  5) Salva propostas em outputs/propostas_novas_features.csv\n",
    "#  6) Faz BACKUP simples do dicionário (features_padronizadas_old.csv) e ACRESCENTA as novas entradas no features_padronizadas.csv\n",
    "#\n",
    "# Observações:\n",
    "#  - 'dimensao' será deixada em branco (\"\") para posterior preenchimento humano, evitando inferências arriscadas.\n",
    "#  - Não altera nenhum arquivo de dados. Só lê quando precisa pegar exemplo de valor.\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import re, unicodedata\n",
    "import shutil\n",
    "\n",
    "# ============== CONFIGURAÇÃO ==============\n",
    "BASE_DIR = Path(r\"C:\\Users\\wilso\\MBA_EMPREENDEDORISMO\\3AGD\\A1_LOCAL\")\n",
    "ARQ_AUDITORIA = BASE_DIR / \"outputs\" / \"auditoria_normalizacao_features.csv\"\n",
    "ARQ_DIC       = BASE_DIR / \"features_padronizadas.csv\"\n",
    "ARQ_DIC_BKP   = BASE_DIR / \"features_padronizadas_old.csv\"\n",
    "ARQ_PROPOSTAS = BASE_DIR / \"outputs\" / \"propostas_novas_features.csv\"\n",
    "\n",
    "# Para buscar exemplo de valor, usaremos estes diretórios (mesmos da auditoria):\n",
    "BUSCA_DIRS = [\n",
    "    BASE_DIR / \"data\" / \"curated\",\n",
    "    BASE_DIR / \"outputs\",\n",
    "    BASE_DIR / \"outputs\" / \"baseline_datasets\",\n",
    "    BASE_DIR / \"outputs\" / \"pedra\",\n",
    "    BASE_DIR / \"outputs\" / \"eda\",\n",
    "    BASE_DIR / \"outputs\" / \"indices\",\n",
    "]\n",
    "\n",
    "# ============== UTILITÁRIOS ==============\n",
    "def norm_key(s: str) -> str:\n",
    "    return (s or \"\").strip().lower()\n",
    "\n",
    "def read_csv_any_encoding(p: Path, nrows=None):\n",
    "    for enc in (\"utf-8\", \"utf-8-sig\", \"latin1\"):\n",
    "        try:\n",
    "            return pd.read_csv(p, encoding=enc, nrows=nrows, low_memory=False), enc\n",
    "        except Exception:\n",
    "            continue\n",
    "    return pd.read_csv(p, nrows=nrows, low_memory=False), None\n",
    "\n",
    "def strip_accents(s: str) -> str:\n",
    "    return \"\".join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c))\n",
    "\n",
    "def snake_clean(s: str) -> str:\n",
    "    s = re.sub(r\"[^a-z0-9_]+\", \"_\", s.lower())\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "# === Regras determinísticas (iguais às usadas antes) ===\n",
    "phrase_rules = [\n",
    "    (r\"total_paf_air_flow\",        \"vazao_ar_prim_total\"),\n",
    "    (r\"total_air_flow\",            \"vazao_ar_total\"),\n",
    "    (r\"main_steam_flow_sel\",       \"vazao_vapor_principal\"),\n",
    "    (r\"main_steam_pressure\",       \"pressao_vapor_principal\"),\n",
    "    (r\"hot_reheat_steam_pressure\", \"pressao_vapor_reaquec\"),\n",
    "    (r\"hot_reheat_steam_temperature\", \"temp_vapor_reaquec\"),\n",
    "    (r\"drum_level_sel\",            \"nivel_tambor\"),\n",
    "    (r\"drum_pres_sel\",             \"pressao_tambor\"),\n",
    "    (r\"furnace_pres\",              \"pressao_fornalha\"),\n",
    "    (r\"feed_water_flw_sel\",        \"vazao_agua_alimentacao\"),\n",
    "    (r\"o2_avg_?$\",                 \"o2_medio\"),\n",
    "    (r\"status_operacao\",           \"status_operacao\"),\n",
    "    (r\"flw_total_a\",               \"vazao_total_a\"),\n",
    "    (r\"flw_total_b\",               \"vazao_total_b\"),\n",
    "    (r\"flw_total_c\",               \"vazao_total_c\"),\n",
    "]\n",
    "\n",
    "token_map = {\n",
    "    \"cur\": \"corr\", \"current\": \"corrente\",\n",
    "    \"prs\": \"pressao\", \"press\": \"pressao\", \"pressure\": \"pressao\",\n",
    "    \"temp\": \"temp\", \"temperature\": \"temp\", \"te\": \"temp\",\n",
    "    \"flw\": \"vazao\", \"flow\": \"vazao\",\n",
    "    \"level\": \"nivel\", \"avg\": \"medio\",\n",
    "\n",
    "    \"fan\": \"vent\", \"mtr\": \"motor\",\n",
    "    \"idf\": \"vent_tiragem\", \"id\": \"vent_tiragem\",\n",
    "    \"pa\": \"ar_prim\", \"sa\": \"ar_sec\",\n",
    "    \"hpfa\": \"ar_flu_ap\", \"hpa\": \"ar_alta_press\",\n",
    "    \"aph\": \"preaq_ar\", \"ltr\": \"ltr\", \"ecmz\": \"econ\",\n",
    "    \"deaerator\": \"desaerador\", \"dea\": \"desaerador\",\n",
    "    \"condensate\": \"condensado\", \"pump\": \"bomba\",\n",
    "    \"furnace\": \"fornalha\", \"bed\": \"leito\", \"drum\": \"tambor\",\n",
    "    \"steam\": \"vapor\", \"water\": \"agua\", \"gas\": \"gas\",\n",
    "\n",
    "    \"inlet\": \"entrada\", \"inl\": \"entrada\",\n",
    "    \"outlet\": \"saida\",  \"outl\": \"saida\",\n",
    "    \"hdr\": \"coletor\", \"pos\": \"pos\", \"fdb\": \"fbk\",\n",
    "\n",
    "    \"upper\": \"sup\", \"lower\": \"inf\", \"medium\": \"medio\",\n",
    "    \"side\": \"lado\", \"a\": \"a\", \"b\": \"b\", \"c\": \"c\",\n",
    "\n",
    "    \"o2\": \"o2\", \"so2\": \"so2\", \"dust\": \"poeira\", \"coal\": \"carvao\",\n",
    "\n",
    "    \"of\": \"\", \"with\": \"\", \"and\": \"\", \"sel\": \"\", \"ol\": \"\", \"mi\": \"\"\n",
    "}\n",
    "\n",
    "def apply_phrase_rules(s: str) -> str:\n",
    "    out = s\n",
    "    for pat, rep in phrase_rules:\n",
    "        out = re.sub(pat, rep, out)\n",
    "    return out\n",
    "\n",
    "def translate_tokens(name: str) -> str:\n",
    "    # remover sufixos de unidade recorrentes no NOME (dimensão fica a cargo humano depois)\n",
    "    name = re.sub(r\"_(t_h|adegc|mpa|kpa|nm3_h|m3_s|rpm|mm|mw|pa)$\", \"\", name)\n",
    "    name = apply_phrase_rules(name)\n",
    "    tokens = [t for t in re.split(r\"_+\", name) if t]\n",
    "    out = []\n",
    "    for t in tokens:\n",
    "        rep = token_map.get(t, t)\n",
    "        for subt in rep.split(\"_\"):\n",
    "            if subt:\n",
    "                out.append(subt)\n",
    "    compact = []\n",
    "    for tok in out:\n",
    "        if not compact or compact[-1] != tok:\n",
    "            compact.append(tok)\n",
    "    candidate = \"_\".join(compact) if compact else \"var\"\n",
    "    candidate = strip_accents(candidate)\n",
    "    candidate = snake_clean(candidate)\n",
    "    return candidate\n",
    "\n",
    "# ============== 1) Ler inventário e coletar NÃO MAPEADAS ==============\n",
    "if not ARQ_AUDITORIA.exists():\n",
    "    raise FileNotFoundError(f\"Inventário de auditoria não encontrado: {ARQ_AUDITORIA}\")\n",
    "if not ARQ_DIC.exists():\n",
    "    raise FileNotFoundError(f\"Dicionário não encontrado: {ARQ_DIC}\")\n",
    "\n",
    "aud = pd.read_csv(ARQ_AUDITORIA, dtype=str).fillna(\"\")\n",
    "\n",
    "# Mapa: coluna_nao_mapeada -> {arquivos onde aparece}\n",
    "unmapped_sources = defaultdict(set)\n",
    "for _, row in aud.iterrows():\n",
    "    arq = row.get(\"arquivo_relativo\", \"\")\n",
    "    raw = row.get(\"orig_nao_mapeadas\", \"\")\n",
    "    if not raw:\n",
    "        continue\n",
    "    for col in [c.strip() for c in raw.split(\";\") if c.strip()]:\n",
    "        unmapped_sources[col].add(arq)\n",
    "\n",
    "lista_unmapped = sorted(unmapped_sources.keys(), key=lambda x: x.lower())\n",
    "print(f\"NÃO mapeadas distintas encontradas: {len(lista_unmapped)}\")\n",
    "\n",
    "if not lista_unmapped:\n",
    "    print(\"Nada a fazer: não há colunas não mapeadas no inventário.\")\n",
    "    # encerra graciosamente\n",
    "    raise SystemExit(0)\n",
    "\n",
    "# ============== 2) Carregar dicionário atual para checar conflitos ==============\n",
    "dic = pd.read_csv(ARQ_DIC, dtype=str).fillna(\"\")\n",
    "dic[\"nome_original\"] = dic[\"nome_original\"].map(norm_key)\n",
    "dic[\"nome_pt\"]       = dic[\"nome_pt\"].astype(str).str.strip()\n",
    "\n",
    "exist_pt = set(dic[\"nome_pt\"].tolist())\n",
    "exist_orig = set(dic[\"nome_original\"].tolist())\n",
    "\n",
    "# ============== 3) Propor nome_pt e coletar exemplo de valor ==============\n",
    "def achar_exemplo_valor(col_original: str, fontes_relativas: set[str]):\n",
    "    \"\"\"\n",
    "    Tenta encontrar um exemplo de valor para 'col_original' procurando nas fontes informadas.\n",
    "    Leitura limitada (nrows=None mas early-stop ao achar um não-nulo).\n",
    "    Match de coluna é case-insensitive e ignora espaços.\n",
    "    \"\"\"\n",
    "    alvo_norm = norm_key(col_original)\n",
    "    # lista de caminhos absolutos candidatos\n",
    "    candidatos = []\n",
    "    for rel in fontes_relativas:\n",
    "        p = BASE_DIR / Path(rel)\n",
    "        if p.exists() and p.suffix.lower()==\".csv\":\n",
    "            candidatos.append(p)\n",
    "\n",
    "    # fallback: procura pelo nome do arquivo em diretórios padrão se caminho relativo não existir\n",
    "    if not candidatos:\n",
    "        for d in BUSCA_DIRS:\n",
    "            for p in d.glob(\"*.csv\"):\n",
    "                # heurística simples: se o arquivo contém a coluna no cabeçalho (nrows=0)\n",
    "                try:\n",
    "                    df_head, _ = read_csv_any_encoding(p, nrows=0)\n",
    "                    cols_norm = [norm_key(c) for c in df_head.columns]\n",
    "                    if alvo_norm in cols_norm:\n",
    "                        candidatos.append(p)\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "    for p in candidatos:\n",
    "        try:\n",
    "            df, _ = read_csv_any_encoding(p)\n",
    "            # localizar a coluna real por normalização\n",
    "            col_match = None\n",
    "            mapa = {norm_key(c): c for c in df.columns}\n",
    "            if alvo_norm in mapa:\n",
    "                col_match = mapa[alvo_norm]\n",
    "            if col_match is None:\n",
    "                continue\n",
    "            serie = df[col_match].dropna()\n",
    "            if not serie.empty:\n",
    "                # pega o primeiro não-nulo como string curta\n",
    "                val = str(serie.iloc[0])\n",
    "                return val[:120]\n",
    "        except Exception:\n",
    "            continue\n",
    "    return \"\"\n",
    "\n",
    "propostas = []\n",
    "seen_sugestoes = set(exist_pt)  # para garantir unicidade vs dicionário atual\n",
    "\n",
    "for orig in lista_unmapped:\n",
    "    orig_norm = norm_key(orig)\n",
    "    if orig_norm in exist_orig:\n",
    "        # Já existe no dicionário (raro, mas defensivo)\n",
    "        continue\n",
    "\n",
    "    sug = translate_tokens(orig_norm)\n",
    "    base = sug\n",
    "    # garantir unicidade com sufixo _2, _3 ... se colidir com existentes\n",
    "    i = 2\n",
    "    while sug in seen_sugestoes:\n",
    "        sug = f\"{base}_{i}\"\n",
    "        i += 1\n",
    "    seen_sugestoes.add(sug)\n",
    "\n",
    "    exemplo = achar_exemplo_valor(orig, unmapped_sources[orig])\n",
    "    propostas.append({\n",
    "        \"nome_original\": orig_norm,\n",
    "        \"sugestao_nome_pt\": sug,\n",
    "        \"fontes\": \";\".join(sorted(unmapped_sources[orig])),\n",
    "        \"exemplo_valor\": exemplo\n",
    "    })\n",
    "\n",
    "df_prop = pd.DataFrame(propostas)\n",
    "ARQ_PROPOSTAS.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_prop.to_csv(ARQ_PROPOSTAS, index=False, encoding=\"utf-8\")\n",
    "print(f\"Propostas salvas em: {ARQ_PROPOSTAS}  |  Novas entradas: {len(df_prop)}\")\n",
    "\n",
    "# ============== 4) Acrescentar ao dicionário (backup simples) ==============\n",
    "if not df_prop.empty:\n",
    "    # backup simples, uma única vez (não sobrescreve se já existir)\n",
    "    if not ARQ_DIC_BKP.exists():\n",
    "        shutil.copy2(ARQ_DIC, ARQ_DIC_BKP)\n",
    "        print(f\"Backup do dicionário criado: {ARQ_DIC_BKP}\")\n",
    "\n",
    "    df_new = pd.DataFrame({\n",
    "        \"nome_original\": df_prop[\"nome_original\"],\n",
    "        \"nome_pt\": df_prop[\"sugestao_nome_pt\"],\n",
    "        \"dimensao\": [\"\" for _ in range(len(df_prop))],       # deixar em branco para preenchimento humano\n",
    "        \"exemplo_valor\": df_prop[\"exemplo_valor\"].fillna(\"\")\n",
    "    })\n",
    "\n",
    "    # anexar ao final, preservando as colunas na ordem correta\n",
    "    dic_cols = [\"nome_original\", \"nome_pt\", \"dimensao\", \"exemplo_valor\"]\n",
    "    df_out = pd.concat([dic[dic_cols], df_new[dic_cols]], ignore_index=True)\n",
    "    df_out.to_csv(ARQ_DIC, index=False, encoding=\"utf-8\")\n",
    "    print(f\"Dicionário atualizado: {ARQ_DIC}  |  Linhas totais: {len(df_out)}\")\n",
    "else:\n",
    "    print(\"Nenhuma proposta gerada; dicionário não foi modificado.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
